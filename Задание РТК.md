К заданию приложен файл с данными в формате csv. Данные содержат логи работы скрипт-менеджера, приложения для обслуживания клиентов компании операторами.

Задача:

1. На основании файла создать таблицу в БД Clickhouse и залить в неё учебные данные. Запросы CREATE и INSERT выложить в GitHub. Дополнительно прокомментировать, какие особенности есть у данных, и как они были учтены.
2. Необходимо написать запрос-преобразование таблицы по номеру скрипта (script_id), вывести все основные параметры, необходимые для дальнейшей работы аналитиков и построения BI-отчётов. Запрос/код выложить в GitHub. (включая создание новой таблицы в CH). Необходимо учесть, что данные в БД должны храниться в нормальной форме, а для подключения к BI Superset должна использоваться только одна таблица.
3. Настроить ETL. Каждый час данные должны выгружаться из источника (таблица CH из п1.), преобразовываться и сохраняться в таблицу CH. В Github выложить код DAG Airflow. (размещения в Github 1,2,3 пунктов выполнить разными commit)

Описание необходимых признаков:
* `timestamp` - Дата и время
* `user` - Логин
* `communication_number` - Номер обращения 
* `communication_id` - ID Вызова
* `script_id` - ID скрипта
* `script_name` - Имя скрипта
* `mrf` - Регион оператора
* `client_mrf` - Регион клиента
* `script_owner` - Разработчик скрипта
* `current_script_owner` - Разработчик вложенного скрипта
* `script_responsible` - Ответственный по скрипту
* `current_script_responsible` - Ответственный по вложенному скрипту
* `crm_departament` - Подразделение
* `ACCOUNT_NUMBER` - Номер лицевого счета
* `CALLER_ID` - Номер телефона
* `COMMUNICATION_THEME` - Тема обращения
* `COMMUNICATION_DETAIL` - Детализация обращения
* `COMMUNICATION_RESULT` - Результат обращения

Часть признаков хранится в parameters